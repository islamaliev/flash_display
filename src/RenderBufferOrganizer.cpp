#include <cstring>
#include <ComponentContainer.h>
#include <Texture.h>
#include "Contex.h"
#include "RenderBufferOrganizer.h"
#include "StackAllocator.h"
#include "DisplayObject.h"
#include "Program.h"

using namespace flash;
using namespace render;

using Mat4 = math::Mat4;
using DisplayObject = display::DisplayObject;
using StackAllocator = allocator::StackAllocator;

namespace {
    class ForEachTextureDataIterator {
    public:
        ForEachTextureDataIterator(int& numLeafComponents, unsigned* batchSizes, unsigned& numDraws)
            : m_numLeafComponents(numLeafComponents)
            , m_batchSizes(batchSizes)
            , m_numDraws(numDraws){}
        
        void operator()(TextureData& textureData, int depth) {
            if (depth <= 0)
                return;
            bool toOverride = m_lastDepth == depth - 1;
            m_lastDepth = depth;
            m_numLeafComponents += !toOverride;
            auto batchIndex = textureData.textureId  >> Context::s_batchBitsNum;
            ++m_batchSizes[batchIndex];
            m_batchSizes[m_lastBatchIndex] -= toOverride;
            m_numDraws = m_numDraws < batchIndex ? batchIndex : m_numDraws;
            m_lastBatchIndex = batchIndex;
        }
        
    private:
        int& m_numLeafComponents;
        unsigned* m_batchSizes;
        unsigned& m_numDraws;
        int m_lastDepth{1};
        unsigned m_lastBatchIndex{0};
    };
    
    class ForEachIterator {
    public:
        ForEachIterator(Mat4* parentMatrices, BufferData& bufData, int* offsets)
            : m_parentMatrices(parentMatrices)
            , m_bufData(bufData)
            , m_offsets(offsets) {}
        
        void operator()(SpatialComponent& spatial, TextureData& textureData, int depth, int order) {
            if (depth <= 0)
                return;
            auto batchIndex = textureData.textureId >> Context::s_batchBitsNum;
            bool toOverride = m_lastDepth == depth - 1;
            // TODO: check if conditional move is used here
            auto index = toOverride && m_lastBatchIndex == batchIndex ? m_lastIndex : m_offsets[batchIndex];
            // current batch offset advances once an object is added
            ++m_offsets[batchIndex];
            m_offsets[m_lastBatchIndex] -= toOverride;
            // for shapes the index is -1 so that it can be checked in ths frag shader
            // all other indices must be from 0 to GL_MAX_TEXTURE_IMAGE_UNITS
            int texUnitIndex = int(textureData.textureId - (batchIndex << Context::s_batchBitsNum));
            texUnitIndex = textureData.textureId ? texUnitIndex : -1;
            m_bufData.textures[index] = texUnitIndex;
            Mat4* m = m_bufData.matrices + index;
            *m = m_parentMatrices[depth] = m_parentMatrices[depth - 1] * DisplayObject::_getTransform(spatial, order);
            // z order of an object is given in global space so we prevent concatenation with parent's z
            m_parentMatrices[depth].zt(0);
            m_lastDepth = depth;
            m_lastIndex = index;
            m_lastBatchIndex = batchIndex;
        }
                
    private:
        Mat4* m_parentMatrices;
        BufferData& m_bufData;
        int* m_offsets;
        int m_lastDepth{1};
        int m_lastIndex{0};
        unsigned m_lastBatchIndex{0};
    };
}

// TODO: find a way to get rid of this MAX_TREE_DEPTH
#define MAX_TREE_DEPTH 100

void RenderBufferOrganizer::organize(flash::display::DisplayObject& stage, StackAllocator& allocator, BufferData& bufData) {
    ComponentContainer& components = stage._getComponents();
    components.sort();
    
    const unsigned numTextures = display::Texture::s_numTextures + 1; // this includes shapes, e.g. objects with no texture
    // TODO: size of allocated memory should be numTextureGroups, not numTextures, nor numTextures >> Context::s_batchBitsNum
    // we don't control which texture id is generated by OpenGL. In the worst case all textures can fall in different groups
    unsigned* batchSizes = (unsigned*) allocator.alloc(sizeof(unsigned) * numTextures);
    memset(batchSizes, 0, sizeof(unsigned) * numTextures);

    int numLeafComponents = 0;
    unsigned numDraws = 0;

    components.forEachTextureData(ForEachTextureDataIterator(numLeafComponents, batchSizes, numDraws));

    // this is a work around. We emulate that first batch has one more object than it needs.
    // It's necessary because container objects also fall into first batch (since theirs texture is 0).
    // Container aren't present in the final result but during calculation we put the in the same array.
    // When, for example, first batch is fully set (and at least fist object from the second batch) then
    // when next container is encountered it's set to the next slot of first batch which is at that moment
    // first element of the second batch (since first one is full). That's why we need additional slot in
    // the first batch. Matrix for this object needs to be set to 0 so that it's not being drawn.
    ++numLeafComponents;
    ++batchSizes[0];
    bufData.batchSizes = batchSizes;
    bufData.numDraws = numDraws + 1;

    // offset for each batch
    int* offsets = (int*) allocator.alloc(sizeof(int) * numTextures);
    offsets[0] = 0;
    for (int i = 1; i < bufData.numDraws; ++i) {
        offsets[i] = offsets[i - 1] + batchSizes[i - 1];
    }

    Mat4* parentMatrices = (Mat4*) allocator.alloc(sizeof(Mat4) * MAX_TREE_DEPTH);
    *parentMatrices = Mat4();

    bufData.matrices = (Mat4*) allocator.alloc(sizeof(Mat4) * numLeafComponents);
    bufData.textures = (int*) allocator.alloc(sizeof(Context::TextureIndexType) * numLeafComponents);

    memset(bufData.matrices + batchSizes[0] - 1, 0, sizeof(Mat4));
    
    components.forEach(ForEachIterator(parentMatrices, bufData, offsets));
}